{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gDJxmN17C5Ld",
        "-B_-3X1MGrWg",
        "O4yRABCIJwft"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMFok14sglm1qjXbWWVQoL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaviCg1/PySpark/blob/main/PySpark_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capitulo 2"
      ],
      "metadata": {
        "id": "gDJxmN17C5Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar SDK Java 8\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Descargar Spark 3.4.3\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\n",
        "\n",
        "# Descomprimir el archivo descargado de Spark\n",
        "\n",
        "!tar xf spark-3.4.3-bin-hadoop3.tgz\n",
        "\n",
        "# Establecer las variables de entorno\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "\n",
        "# Instalar la librería findspark\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "# Instalar pyspark\n",
        "\n",
        "!pip install -q pyspark\n",
        "\n",
        "### verificar la instalación ###\n",
        "\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Probando la sesión de Spark\n",
        "df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n",
        "\n",
        "df.show(10, False)"
      ],
      "metadata": {
        "id": "XZRJcWU_zTs2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7541e5-6400-48d1-96ee-4e63f92725ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+-----+\n",
            "|Hola |\n",
            "+-----+\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "|Mundo|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capitulo 3 Inroduccion"
      ],
      "metadata": {
        "id": "iVCSOuzO3O3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#Se crea la sesion y se le da el numero de cores en master y el nombre que va a tener\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('Curso Pyspark').getOrCreate()\n",
        "\n",
        "spark\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "IbxeFIq43NLH",
        "outputId": "144fe04c-3d24-4b82-f67d-ba0a217d69ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c7322a2cdf0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://405ef383626c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diferentes formas de crear un RDD\n",
        "\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Crear un RDD vacío\n",
        "\n",
        "rdd_vacio = sc.emptyRDD\n",
        "\n",
        "# Crear un RDD con parallelize\n",
        "\n",
        "rdd_vacio3 = sc.parallelize([], 3)\n",
        "\n",
        "rdd_vacio3.getNumPartitions()\n",
        "\n",
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "\n",
        "rdd.collect()\n",
        "\n",
        "# Crear un RDD desde un archivo de texto\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TsCuFD84_6x",
        "outputId": "1e70bf5f-d58b-4eeb-982b-1b880ed1c2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importa y crea un rdd en base a archivos de texto y sus lineas\n",
        "rdd_texto = sc.textFile('./rdd_source.txt')\n",
        "\n",
        "rdd_texto.collect()\n",
        "\n",
        "#A un rdd se le puede añadir un lambda para sumar valores o añadir cosas\n",
        "rdd_suma = rdd.map(lambda x: x +1)\n",
        "\n",
        "rdd_suma.collect()\n",
        "\n",
        "#Aqui creamos un dataframe\n",
        "df = spark.createDataFrame([(1, 'jose'), (2, 'juan')], ['id', 'nombre'])\n",
        "\n",
        "df.show()\n",
        "\n",
        "#Creas un rdd del dataframe\n",
        "rdd_df = df.rdd\n",
        "\n",
        "rdd_df.collect()\n",
        "\n",
        "# Leer el archivo CSV como un DataFrame\n",
        "df1 = spark.read.csv(\"rdd_source.txt\", header=True, inferSchema=True)\n",
        "\n",
        "# Mostrar el contenido del DataFrame\n",
        "df1.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ypu_QMpd5sf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "708372ca-3f6f-425f-ff09-a3d5895f6b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|nombre|\n",
            "+---+------+\n",
            "|  1|  jose|\n",
            "|  2|  juan|\n",
            "+---+------+\n",
            "\n",
            "+---+-------+----+---------+\n",
            "| id| nombre|edad|   ciudad|\n",
            "+---+-------+----+---------+\n",
            "|  1|   Juan|  28|   Madrid|\n",
            "|  2|  María|  22|Barcelona|\n",
            "|  3|  Pedro|  35|  Sevilla|\n",
            "|  4|    Ana|  30| Valencia|\n",
            "|  5|   Luis|  40|   Bilbao|\n",
            "|  6|  Sofía|  25|   Málaga|\n",
            "|  7| Andrés|  33| Zaragoza|\n",
            "|  8|  Lucía|  29|Santander|\n",
            "|  9|Roberto|  45|  Granada|\n",
            "| 10|  Elena|  27| Alicante|\n",
            "+---+-------+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicios de prueba"
      ],
      "metadata": {
        "id": "IQSUC_3x_SEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppp = sc.textFile('./rdd_source.txt')\n",
        "print(\"Contenido del RDD:\")\n",
        "\n",
        "\n",
        "print(f\"Número de particiones: {ppp.getNumPartitions()}\")\n",
        "ppp.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8rEfbB07ZvI",
        "outputId": "3ca71587-249d-4517-f804-ca96c188cc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenido del RDD:\n",
            "Número de particiones: 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id,nombre,edad,ciudad',\n",
              " '1,Juan,28,Madrid',\n",
              " '2,María,22,Barcelona',\n",
              " '3,Pedro,35,Sevilla',\n",
              " '4,Ana,30,Valencia',\n",
              " '5,Luis,40,Bilbao',\n",
              " '6,Sofía,25,Málaga',\n",
              " '7,Andrés,33,Zaragoza',\n",
              " '8,Lucía,29,Santander',\n",
              " '9,Roberto,45,Granada',\n",
              " '10,Elena,27,Alicante']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capitulo 4 Transoformaciones RDD"
      ],
      "metadata": {
        "id": "-iONoos__adc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformaciones: función map\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C8py7j_XC1pZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "2c6ab353-efde-4de1-b78b-44985040a9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'findspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f11dd85986ea>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Transformaciones: función map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Map"
      ],
      "metadata": {
        "id": "-B_-3X1MGrWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "\n",
        "rdd_resta = rdd.map(lambda x: x - 1)\n",
        "rdd_resta.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRqV-L8_Fpy-",
        "outputId": "22b7831b-a0db-411e-e633-16bcf78be969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_par = rdd.map(lambda x: x % 2 == 0)\n",
        "\n",
        "rdd_par.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM9s0AlzFlbR",
        "outputId": "65b061b5-2f20-46d8-ce05-aef8aa104d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False, True, False, True, False]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_texto = sc.parallelize(['jose', 'juan', 'lucia'])\n",
        "\n",
        "rdd_mayuscula = rdd_texto.map(lambda x: x.upper())\n",
        "\n",
        "rdd_mayuscula.collect()\n",
        "\n",
        "rdd_hola = rdd_mayuscula.map(lambda x: 'Hola ' + x)\n",
        "\n",
        "rdd_hola.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynF-98aFFssH",
        "outputId": "55fd73d1-fcdf-437d-8767-864fef9e0236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola JOSE', 'Hola JUAN', 'Hola LUCIA']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FlatMap"
      ],
      "metadata": {
        "id": "n1mSgkWrGvCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5])\n",
        "\n",
        "#Esta funcion devuelve otro rrd con dos partes: x , x^2\n",
        "rdd_cuadrado = rdd.map(lambda x: (x, x ** 2))\n",
        "\n",
        "\n",
        "rdd_cuadrado.collect()\n",
        "#rdd_cuadrado.getNumPartitions()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZI_sltWGz-5",
        "outputId": "3b104435-4c6f-4ff0-a79e-0d58a205eeff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transoforma el rdd en un map plano (Transforma los objetos en una lista plana)\n",
        "rdd_cuadrado_flat = rdd.flatMap(lambda x: (x, x ** 2))\n",
        "\n",
        "rdd_cuadrado_flat.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcvpwvTEG0Tu",
        "outputId": "a2472f4d-953b-4d1c-a6e5-ec97625539d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 2, 4, 3, 9, 4, 16, 5, 25]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_texto = sc.parallelize(['jose', 'juan', 'lucia'])\n",
        "\n",
        "rdd_mayuscula = rdd_texto.flatMap(lambda x: (x, x.upper()))\n",
        "\n",
        "rdd_mayuscula.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wwEfhZJG0fb",
        "outputId": "fc45e724-dd59-4f81-88e9-095a022d687f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jose', 'JOSE', 'juan', 'JUAN', 'lucia', 'LUCIA']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter"
      ],
      "metadata": {
        "id": "4MAkohegIKwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
        "\n",
        "#Con map hubiera devuleto el resultado y aqui devuelve los valores que sean true\n",
        "rdd_par = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "rdd_par.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1cnC3GsIPnj",
        "outputId": "3eafbcd4-161b-4f65-f97d-9102f17b41a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_impar = rdd.filter(lambda x: x % 2 != 0)\n",
        "\n",
        "rdd_impar.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH9CFhHgIPaz",
        "outputId": "83f1edec-17c0-4a4b-f0e1-1f0910fb2262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 5, 7, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_texto = sc.parallelize(['jose', 'juaquin', 'juan', 'lucia', 'karla', 'katia'])\n",
        "\n",
        "rdd_k = rdd_texto.filter(lambda x: x.startswith('p'))\n",
        "\n",
        "rdd_k.collect()\n",
        "\n"
      ],
      "metadata": {
        "id": "6_rtn_zaIPLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_filtro = rdd_texto.filter(lambda x: x.startswith('j') and x.find('u') == 1)\n",
        "\n",
        "rdd_filtro.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIQ6G5rLJPsb",
        "outputId": "d468c04c-8dd7-4064-ec16-1fd8f238d06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['juaquin', 'juan']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##coalesce"
      ],
      "metadata": {
        "id": "O4yRABCIJwft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1,2,3.4,5], 10)\n",
        "\n",
        "rdd.getNumPartitions()\n",
        "#Junta particiones y las fusiona\n",
        "\n",
        "rdd1=rdd.coalesce(5)\n",
        "rdd1.getNumPartitions()\n",
        "rdd1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HruzGukeJx8q",
        "outputId": "1020a9a8-b194-406e-9428-d0ca86f53fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3.4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.getNumPartitions()\n",
        "\n",
        "rdd5 = rdd.coalesce(5)\n",
        "\n",
        "rdd5.getNumPartitions()"
      ],
      "metadata": {
        "id": "xBcz5PKJJ1sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Repartition"
      ],
      "metadata": {
        "id": "o9--SQxBLB2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Incrementa el numero de particiones, coalesce es mas optima para reducir el numero\n",
        "rdd = sc.parallelize([1,2,3,4,5], 3)\n",
        "\n",
        "rdd.getNumPartitions()\n",
        "\n",
        "rdd7 = rdd.repartition(7)\n",
        "\n",
        "rdd7.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rewUVnynLBB7",
        "outputId": "266137f4-4cf6-4b56-e920-53cb5563f17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ReducedByKey"
      ],
      "metadata": {
        "id": "lhsPeY0GLows"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimiza los datos de las particiones en caso de tener la misma key\n",
        "rdd = sc.parallelize(\n",
        "    [('casa', 1),\n",
        "     ('parque', 1),\n",
        "     ('que', 5),\n",
        "     ('casa', 1),\n",
        "     ('escuela', 2),\n",
        "     ('casa', 2),\n",
        "     ('que', 1)]\n",
        ")\n",
        "\n",
        "rdd.collect()\n",
        "\n",
        "#rdd.getNumPartitions()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyGUi2lGLoOD",
        "outputId": "26272726-2b36-499c-f15a-ed22dfabb5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('casa', 1),\n",
              " ('parque', 1),\n",
              " ('que', 5),\n",
              " ('casa', 1),\n",
              " ('escuela', 2),\n",
              " ('casa', 2),\n",
              " ('que', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_reduciodo = rdd.reduceByKey(lambda x,y: x + y)\n",
        "\n",
        "rdd_reduciodo.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5ZuKf3cJxxs",
        "outputId": "f12d3b7e-d768-4397-a787-b55eed76fbc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('parque', 1), ('que', 6), ('casa', 4), ('escuela', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ejercicios"
      ],
      "metadata": {
        "id": "pMqMvIusVHEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "JM7OpHWqVbyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(['piton', 'scala', 'spark'])\n",
        "rddMayus= rdd.map(lambda x: x.upper())\n",
        "\n",
        "rddMayus.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SjQb4HDVIlZ",
        "outputId": "4274b989-60bf-4e0d-c494-81e4079be65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PITON', 'SCALA', 'SPARK']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rddP = rdd.filter(lambda x: x.startswith('p'))\n",
        "rddP.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD6aSGSVV5U7",
        "outputId": "fd4c41df-0265-4835-fe09-fa9e2e5b82b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['piton']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([50,51,52,53,54,55,56,57,58,59])\n",
        "rddRaiz= rdd.flatMap(lambda x: (x, x ** 0.5))\n",
        "rdd7 = rddRaiz.repartition(7)\n",
        "rdd7.collect()\n",
        "rdd7.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O9Mrh_y5GWZ",
        "outputId": "1fc39ab7-30c0-4362-ba69-c705bbb7069e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_trans = sc.textFile('./transacciones')\n",
        "\n",
        "rdd_redu = rdd_trans.reduceByKey(lambda x,y: x + y)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "0U6xaVws7Oj-",
        "outputId": "7ed5da58-3845-4645-9902-97d451abb474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4e0f12c20789>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./transacciones'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrdd_redu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Capitulo 5"
      ],
      "metadata": {
        "id": "4AEYGRIaXAys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "s49RIlptW__K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reduce"
      ],
      "metadata": {
        "id": "wH-MIs99XIhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([2,4,6,8])\n",
        "#reduce el rdd entero y lo manda al drive y este si que hace que se ejecute\n",
        "rdd.reduce(lambda x,y: x + y)\n",
        "\n",
        "rdd1 = sc.parallelize([1,2,3,4])\n",
        "\n",
        "rdd1.reduce(lambda x,y: x * y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AF2tcbAXFgS",
        "outputId": "437de342-f0ad-4abd-c165-44e465102089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Count"
      ],
      "metadata": {
        "id": "GKt3xyaeabvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(['j', 'o', 's', 'e'])\n",
        "#Cuenta\n",
        "rdd.count()\n",
        "\n",
        "rdd1 = sc.parallelize([item for item in range(10)])\n",
        "\n",
        "rdd1.count()\n",
        "\n",
        "# Elevar al cuadrado los números\n",
        "# [item ** 2 for item in range(10)]\n",
        "\n",
        "# Filtrar para obtener solo los números pares\n",
        "# [item for item in range(10) if item % 2 == 0]\n",
        "\n",
        "# Filtrar para obtener solo los números impares\n",
        "# [item for item in range(10) if item % 2 != 0]\n",
        "\n",
        "# Crear tuplas con el número y su cuadrado\n",
        "# [(item, item ** 2) for item in range(10)]\n",
        "\n",
        "# Filtrar para obtener solo números impares mayores que 5\n",
        "# [item for item in range(10) if item % 2 != 0 and item > 5]\n",
        "\n",
        "# Aplicar una función a cada número de la lista\n",
        "# def f(x):\n",
        "#     return x + 10\n",
        "# [f(item) for item in range(10)]\n",
        "\n",
        "# Anidar comprehensions para crear pares de números\n",
        "# [[x, y] for x in range(3) for y in range(3)]\n",
        "\n",
        "# Convertir los números a cadenas de texto\n",
        "# [str(item) for item in range(10)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRubdF9fabFE",
        "outputId": "961603d0-6bba-405d-d687-2f520a786bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Collect"
      ],
      "metadata": {
        "id": "ERntU9M4btvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize('Hola Apache Spark!'.split(' '))\n",
        "\n",
        "rdd.collect()\n",
        "\n",
        "rdd1 = sc.parallelize([(item, item ** 2) for item in range(20)])\n",
        "\n",
        "rdd1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUxyGUs-bwQE",
        "outputId": "750caf95-422e-4d2c-da00-33a5ee5a5b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0),\n",
              " (1, 1),\n",
              " (2, 4),\n",
              " (3, 9),\n",
              " (4, 16),\n",
              " (5, 25),\n",
              " (6, 36),\n",
              " (7, 49),\n",
              " (8, 64),\n",
              " (9, 81),\n",
              " (10, 100),\n",
              " (11, 121),\n",
              " (12, 144),\n",
              " (13, 169),\n",
              " (14, 196),\n",
              " (15, 225),\n",
              " (16, 256),\n",
              " (17, 289),\n",
              " (18, 324),\n",
              " (19, 361)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DmTZr_wBcDf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Take max y algo mas"
      ],
      "metadata": {
        "id": "kqO0oeKLdBzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take\n",
        "\n",
        "rdd = sc.parallelize('La programación es bella'.split(' '))\n",
        "\n",
        "rdd.take(2)\n",
        "\n",
        "rdd.take(4)\n",
        "\n",
        "# max\n",
        "\n",
        "rdd1 = sc.parallelize([item/(item + 1) for item in range(10)])\n",
        "\n",
        "rdd1.max()\n",
        "\n",
        "rdd1.collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXc4rbFTdEjG",
        "outputId": "6e22dc01-2a87-40b7-ac63-b0401e995b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.5,\n",
              " 0.6666666666666666,\n",
              " 0.75,\n",
              " 0.8,\n",
              " 0.8333333333333334,\n",
              " 0.8571428571428571,\n",
              " 0.875,\n",
              " 0.8888888888888888,\n",
              " 0.9]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ejercicios"
      ],
      "metadata": {
        "id": "r8Z5j5uveiLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Calcula el factorial de 5\n",
        "\n",
        "rdd = sc.parallelize([item for item in range(100)])\n",
        "rdd1 = rdd.map(lambda x: (x, math.factorial(x)))\n",
        "rdd1.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNzcidSTekqU",
        "outputId": "e4a27cf1-0e69-49d9-b811-aa0256007e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1),\n",
              " (1, 1),\n",
              " (2, 2),\n",
              " (3, 6),\n",
              " (4, 24),\n",
              " (5, 120),\n",
              " (6, 720),\n",
              " (7, 5040),\n",
              " (8, 40320),\n",
              " (9, 362880),\n",
              " (10, 3628800),\n",
              " (11, 39916800),\n",
              " (12, 479001600),\n",
              " (13, 6227020800),\n",
              " (14, 87178291200),\n",
              " (15, 1307674368000),\n",
              " (16, 20922789888000),\n",
              " (17, 355687428096000),\n",
              " (18, 6402373705728000),\n",
              " (19, 121645100408832000),\n",
              " (20, 2432902008176640000),\n",
              " (21, 51090942171709440000),\n",
              " (22, 1124000727777607680000),\n",
              " (23, 25852016738884976640000),\n",
              " (24, 620448401733239439360000),\n",
              " (25, 15511210043330985984000000),\n",
              " (26, 403291461126605635584000000),\n",
              " (27, 10888869450418352160768000000),\n",
              " (28, 304888344611713860501504000000),\n",
              " (29, 8841761993739701954543616000000),\n",
              " (30, 265252859812191058636308480000000),\n",
              " (31, 8222838654177922817725562880000000),\n",
              " (32, 263130836933693530167218012160000000),\n",
              " (33, 8683317618811886495518194401280000000),\n",
              " (34, 295232799039604140847618609643520000000),\n",
              " (35, 10333147966386144929666651337523200000000),\n",
              " (36, 371993326789901217467999448150835200000000),\n",
              " (37, 13763753091226345046315979581580902400000000),\n",
              " (38, 523022617466601111760007224100074291200000000),\n",
              " (39, 20397882081197443358640281739902897356800000000),\n",
              " (40, 815915283247897734345611269596115894272000000000),\n",
              " (41, 33452526613163807108170062053440751665152000000000),\n",
              " (42, 1405006117752879898543142606244511569936384000000000),\n",
              " (43, 60415263063373835637355132068513997507264512000000000),\n",
              " (44, 2658271574788448768043625811014615890319638528000000000),\n",
              " (45, 119622220865480194561963161495657715064383733760000000000),\n",
              " (46, 5502622159812088949850305428800254892961651752960000000000),\n",
              " (47, 258623241511168180642964355153611979969197632389120000000000),\n",
              " (48, 12413915592536072670862289047373375038521486354677760000000000),\n",
              " (49, 608281864034267560872252163321295376887552831379210240000000000),\n",
              " (50, 30414093201713378043612608166064768844377641568960512000000000000),\n",
              " (51, 1551118753287382280224243016469303211063259720016986112000000000000),\n",
              " (52, 80658175170943878571660636856403766975289505440883277824000000000000),\n",
              " (53, 4274883284060025564298013753389399649690343788366813724672000000000000),\n",
              " (54,\n",
              "  230843697339241380472092742683027581083278564571807941132288000000000000),\n",
              " (55,\n",
              "  12696403353658275925965100847566516959580321051449436762275840000000000000),\n",
              " (56,\n",
              "  710998587804863451854045647463724949736497978881168458687447040000000000000),\n",
              " (57,\n",
              "  40526919504877216755680601905432322134980384796226602145184481280000000000000),\n",
              " (58,\n",
              "  2350561331282878571829474910515074683828862318181142924420699914240000000000000),\n",
              " (59,\n",
              "  138683118545689835737939019720389406345902876772687432540821294940160000000000000),\n",
              " (60,\n",
              "  8320987112741390144276341183223364380754172606361245952449277696409600000000000000),\n",
              " (61,\n",
              "  507580213877224798800856812176625227226004528988036003099405939480985600000000000000),\n",
              " (62,\n",
              "  31469973260387937525653122354950764088012280797258232192163168247821107200000000000000),\n",
              " (63,\n",
              "  1982608315404440064116146708361898137544773690227268628106279599612729753600000000000000),\n",
              " (64,\n",
              "  126886932185884164103433389335161480802865516174545192198801894375214704230400000000000000),\n",
              " (65,\n",
              "  8247650592082470666723170306785496252186258551345437492922123134388955774976000000000000000),\n",
              " (66,\n",
              "  544344939077443064003729240247842752644293064388798874532860126869671081148416000000000000000),\n",
              " (67,\n",
              "  36471110918188685288249859096605464427167635314049524593701628500267962436943872000000000000000),\n",
              " (68,\n",
              "  2480035542436830599600990418569171581047399201355367672371710738018221445712183296000000000000000),\n",
              " (69,\n",
              "  171122452428141311372468338881272839092270544893520369393648040923257279754140647424000000000000000),\n",
              " (70,\n",
              "  11978571669969891796072783721689098736458938142546425857555362864628009582789845319680000000000000000),\n",
              " (71,\n",
              "  850478588567862317521167644239926010288584608120796235886430763388588680378079017697280000000000000000),\n",
              " (72,\n",
              "  61234458376886086861524070385274672740778091784697328983823014963978384987221689274204160000000000000000),\n",
              " (73,\n",
              "  4470115461512684340891257138125051110076800700282905015819080092370422104067183317016903680000000000000000),\n",
              " (74,\n",
              "  330788544151938641225953028221253782145683251820934971170611926835411235700971565459250872320000000000000000),\n",
              " (75,\n",
              "  24809140811395398091946477116594033660926243886570122837795894512655842677572867409443815424000000000000000000),\n",
              " (76,\n",
              "  1885494701666050254987932260861146558230394535379329335672487982961844043495537923117729972224000000000000000000),\n",
              " (77,\n",
              "  145183092028285869634070784086308284983740379224208358846781574688061991349156420080065207861248000000000000000000),\n",
              " (78,\n",
              "  11324281178206297831457521158732046228731749579488251990048962825668835325234200766245086213177344000000000000000000),\n",
              " (79,\n",
              "  894618213078297528685144171539831652069808216779571907213868063227837990693501860533361810841010176000000000000000000),\n",
              " (80,\n",
              "  71569457046263802294811533723186532165584657342365752577109445058227039255480148842668944867280814080000000000000000000),\n",
              " (81,\n",
              "  5797126020747367985879734231578109105412357244731625958745865049716390179693892056256184534249745940480000000000000000000),\n",
              " (82,\n",
              "  475364333701284174842138206989404946643813294067993328617160934076743994734899148613007131808479167119360000000000000000000),\n",
              " (83,\n",
              "  39455239697206586511897471180120610571436503407643446275224357528369751562996629334879591940103770870906880000000000000000000),\n",
              " (84,\n",
              "  3314240134565353266999387579130131288000666286242049487118846032383059131291716864129885722968716753156177920000000000000000000),\n",
              " (85,\n",
              "  281710411438055027694947944226061159480056634330574206405101912752560026159795933451040286452340924018275123200000000000000000000),\n",
              " (86,\n",
              "  24227095383672732381765523203441259715284870552429381750838764496720162249742450276789464634901319465571660595200000000000000000000),\n",
              " (87,\n",
              "  2107757298379527717213600518699389595229783738061356212322972511214654115727593174080683423236414793504734471782400000000000000000000),\n",
              " (88,\n",
              "  185482642257398439114796845645546284380220968949399346684421580986889562184028199319100141244804501828416633516851200000000000000000000),\n",
              " (89,\n",
              "  16507955160908461081216919262453619309839666236496541854913520707833171034378509739399912570787600662729080382999756800000000000000000000),\n",
              " (90,\n",
              "  1485715964481761497309522733620825737885569961284688766942216863704985393094065876545992131370884059645617234469978112000000000000000000000),\n",
              " (91,\n",
              "  135200152767840296255166568759495142147586866476906677791741734597153670771559994765685283954750449427751168336768008192000000000000000000000),\n",
              " (92,\n",
              "  12438414054641307255475324325873553077577991715875414356840239582938137710983519518443046123837041347353107486982656753664000000000000000000000),\n",
              " (93,\n",
              "  1156772507081641574759205162306240436214753229576413535186142281213246807121467315215203289516844845303838996289387078090752000000000000000000000),\n",
              " (94,\n",
              "  108736615665674308027365285256786601004186803580182872307497374434045199869417927630229109214583415458560865651202385340530688000000000000000000000),\n",
              " (95,\n",
              "  10329978488239059262599702099394727095397746340117372869212250571234293987594703124871765375385424468563282236864226607350415360000000000000000000000),\n",
              " (96,\n",
              "  991677934870949689209571401541893801158183648651267795444376054838492222809091499987689476037000748982075094738965754305639874560000000000000000000000),\n",
              " (97,\n",
              "  96192759682482119853328425949563698712343813919172976158104477319333745612481875498805879175589072651261284189679678167647067832320000000000000000000000),\n",
              " (98,\n",
              "  9426890448883247745626185743057242473809693764078951663494238777294707070023223798882976159207729119823605850588608460429412647567360000000000000000000000),\n",
              " (99,\n",
              "  933262154439441526816992388562667004907159682643816214685929638952175999932299156089414639761565182862536979208272237582511852109168640000000000000000000000)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIMoDKrUelCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Capitulo 6\n"
      ],
      "metadata": {
        "id": "wCIFihn6-ocW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Almacenamiento en caché\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "zE4q-Wm7-wmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se selecciona donde se guarda la informacion si en disco o en memoria. Memoria es si entra es mucho mejor\n",
        "rdd = sc.parallelize([item for item in range(10)])\n",
        "\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "rdd.persist(StorageLevel.MEMORY_ONLY)\n",
        "\n",
        "rdd.unpersist()\n",
        "\n",
        "rdd.persist(StorageLevel.DISK_ONLY)\n",
        "\n",
        "rdd.unpersist()\n",
        "\n",
        "rdd.cache()"
      ],
      "metadata": {
        "id": "O-b9R_g5-scD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Organizacion de las particiones y su creacion"
      ],
      "metadata": {
        "id": "fy53E1DaA29U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(['x', 'y', 'z'])\n",
        "\n",
        "hola = 'Hola'\n",
        "\n",
        "hash(hola)\n",
        "\n",
        "num_particiones = 6\n",
        "\n",
        "# indice = hash(item) % num_particiones\n",
        "\n",
        "hash('x') % num_particiones\n",
        "\n",
        "hash('y') % num_particiones\n",
        "\n",
        "hash('z') % num_particiones"
      ],
      "metadata": {
        "id": "DfqhsxwFCuqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Variable Broadcas"
      ],
      "metadata": {
        "id": "WzfDtksJCvkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([item for item in range(10)])\n",
        "\n",
        "uno = 1\n",
        "\n",
        "br_uno = sc.broadcast(uno)\n",
        "\n",
        "rdd1 = rdd.map(lambda x: x + br_uno.value)\n",
        "\n",
        "rdd1.collect()\n",
        "\n",
        "br_uno.unpersist()\n",
        "\n",
        "rdd1  = rdd.map(lambda x: x + br_uno.value)\n",
        "\n",
        "rdd1.collect()\n",
        "\n",
        "br_uno.destroy()\n",
        "\n",
        "rdd1  = rdd.map(lambda x: x + br_uno.value)\n",
        "\n",
        "rdd1.take(5)"
      ],
      "metadata": {
        "id": "oRIfqAn2CwAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Acumuladores"
      ],
      "metadata": {
        "id": "Q5Mwhcs2DWnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acumulador = sc.accumulator(0)\n",
        "\n",
        "rdd = sc.parallelize([2,4,6,8,10])\n",
        "\n",
        "rdd.foreach(lambda x: acumulador.add(x))\n",
        "\n",
        "print(acumulador.value)\n",
        "\n",
        "rdd1 = sc.parallelize('Mi nombre es Jose Miguel y me siento genial'.split(' '))\n",
        "\n",
        "acumulador1 = sc.accumulator(0)\n",
        "\n",
        "rdd1.foreach(lambda x: acumulador1.add(1))\n",
        "\n",
        "print(acumulador1.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SwnCxuCDTJ-",
        "outputId": "76729da4-b9f1-4de6-b5eb-edbc8758ae83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Capitulo 7 DataFrame"
      ],
      "metadata": {
        "id": "nCGCesAdHr9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creando DataFrames\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "fxj1X2b9fi5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, \"x ** 2\"))\n",
        "\n",
        "rdd.collect()\n",
        "\n",
        "#pasa de rdd a dataframe con dos columnas\n",
        "df = rdd.toDF(['numero', 'cudrado'])\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJZ-lcfAH30o",
        "outputId": "c3d6cbe6-05cb-4fdd-8d40-80eeab4ee04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- numero: long (nullable = true)\n",
            " |-- cudrado: string (nullable = true)\n",
            "\n",
            "+------+-------+\n",
            "|numero|cudrado|\n",
            "+------+-------+\n",
            "|     0| x ** 2|\n",
            "|     1| x ** 2|\n",
            "|     2| x ** 2|\n",
            "|     3| x ** 2|\n",
            "|     4| x ** 2|\n",
            "|     5| x ** 2|\n",
            "|     6| x ** 2|\n",
            "|     7| x ** 2|\n",
            "|     8| x ** 2|\n",
            "|     9| x ** 2|\n",
            "+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pasar de rdd a dataframe"
      ],
      "metadata": {
        "id": "cJl1B-4ntJkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame a partir de un RDD con schema\n",
        "\n",
        "rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])\n",
        "\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
        "\n",
        "# Primera vía\n",
        "\n",
        "esquema1 = StructType(\n",
        "    [\n",
        "     StructField('id', IntegerType(), True),\n",
        "     StructField('nombre', StringType(), True),\n",
        "     StructField('saldo', DoubleType(), True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Segunda vía\n",
        "\n",
        "esquema2 = \"`id` INT, `nombre` STRING, `saldo` DOUBLE\"\n",
        "\n",
        "df1 = spark.createDataFrame(rdd1, schema=esquema1)\n",
        "\n",
        "df1.printSchema()\n",
        "\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame(rdd1, schema=esquema2)\n",
        "\n",
        "df2.printSchema()\n",
        "\n",
        "df2.show()\n",
        "\n",
        "# Crear un DataFrame a partir de un rango de números\n",
        "\n",
        "spark.range(5).toDF('id').show()\n",
        "\n",
        "spark.range(3, 15).toDF('id').show()\n",
        "\n",
        "spark.range(0, 20, 2).toDF('id').show()\n"
      ],
      "metadata": {
        "id": "1kCdnDGMggjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Crear dataframe en base a datos"
      ],
      "metadata": {
        "id": "ARKS26grlsbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lee los datos de un txt y crea el df\n",
        "df = spark.read.text('./data/dataTXT.txt')\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Crear un DataFrame mediante la lectura de un archivo csv\n",
        "\n",
        "df1 = spark.read.csv('./data/dataCSV.csv')\n",
        "\n",
        "df1.show()\n",
        "\n",
        "#Le decaramos que la primera fila es el nombre de la columna\n",
        "df1 = spark.read.option('header', 'true').csv('./data/dataCSV.csv')\n",
        "\n",
        "df1.show()\n",
        "\n",
        "# Leer un archivo de texto con un delimitador diferente comoo el |\n",
        "\n",
        "df2 = spark.read.option('header', 'true').option('delimiter', '|').csv('./data/dataTab.txt')\n",
        "\n",
        "df2.show()\n",
        "\n",
        "# Crear un DataFrame a partir de un json proporcionando un schema\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
        "\n",
        "json_schema =  StructType(\n",
        "    [\n",
        "     StructField('color', StringType(), True),\n",
        "     StructField('edad', IntegerType(), True),\n",
        "     StructField('fecha', DateType(), True),\n",
        "     StructField('pais', StringType(), True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "df4 = spark.read.schema(json_schema).json('./data/dataJSON.json')\n",
        "\n",
        "df4.show()\n",
        "\n",
        "df4.printSchema()\n",
        "\n",
        "# Crear un DataFrame a partir de un archivo parquet\n",
        "\n",
        "df5 = spark.read.parquet('./data/dataPARQUET.parquet')\n",
        "\n",
        "df5.show()\n",
        "\n",
        "# Otra alternativa para leer desde una fuente de datos parquet en este caso\n",
        "\n",
        "df6 = spark.read.format('parquet').load('./data/dataPARQUET.parquet')\n",
        "\n",
        "df6.printSchema()"
      ],
      "metadata": {
        "id": "Tys3Nvm4lxT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Columnas"
      ],
      "metadata": {
        "id": "8OvFZVZLt_RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('./data/dataPARQUET.parquet')\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "# Primera alternativa para referirnos a las columnas\n",
        "\n",
        "df.select('title').show()\n",
        "\n",
        "# Segunda alternativa\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.select(col('title')).show()"
      ],
      "metadata": {
        "id": "r9_mXHaluBFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Select"
      ],
      "metadata": {
        "id": "l0j2nCiTJKD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('./data/datos.parquet')\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.select(col('video_id')).show()\n",
        "\n",
        "df.select('video_id', 'trending_date').show()\n",
        "\n",
        "# Esta vía nos dará error\n",
        "\n",
        "df.select(\n",
        "    'likes',\n",
        "    'dislikes',\n",
        "    ('likes' - 'dislikes')\n",
        ").show()\n",
        "\n",
        "# Forma correcta\n",
        "#Para realizar tus propias operacioens\n",
        "df.select(\n",
        "    col('likes'),\n",
        "    col('dislikes'),\n",
        "    (col('likes') - col('dislikes')).alias('aceptacion')\n",
        ").show()\n",
        "\n",
        "# selectExpr\n",
        "\n",
        "df.selectExpr('likes', 'dislikes', '(likes - dislikes) as aceptacion').show()\n",
        "\n",
        "df.selectExpr(\"count(distinct(video_id)) as videos\").show()"
      ],
      "metadata": {
        "id": "cm6F99pkJLhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Filter"
      ],
      "metadata": {
        "id": "WgPPh3_lMCIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('./data/datos.parquet')\n",
        "\n",
        "# filter\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.show()\n",
        "#Aparezca solo lass filas  videos id con ese id\n",
        "df.filter(col('video_id') == '2kyS6SvSYSE').show()\n",
        "\n",
        "#Muy parecido a filter el where\n",
        "df1 = spark.read.parquet('./data/datos.parquet').where(col('trending_date') != '17.14.11')\n",
        "\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.read.parquet('./data/datos.parquet').where(col('likes') > 5000)\n",
        "\n",
        "df2.filter((col('trending_date') != '17.14.11') & (col('likes') > 7000)).show()\n",
        "\n",
        "df2.filter(col('trending_date') != '17.14.11').filter(col('likes') > 7000).show()"
      ],
      "metadata": {
        "id": "E9kfvP0NMEPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DropDuplicates y distinct"
      ],
      "metadata": {
        "id": "cPARUvw9R1Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('./data')\n",
        "\n",
        "# distinct\n",
        "#Elimina duplicados\n",
        "df_sin_duplicados = df.distinct()\n",
        "\n",
        "print('El conteo del dataframe original es {}'.format(df.count()))\n",
        "print('El conteo del dataframe sin duplicados es {}'.format(df_sin_duplicados.count()))\n",
        "\n",
        "# función dropDuplicates\n",
        "\n",
        "dataframe = spark.createDataFrame([(1, 'azul', 567), (2, 'rojo', 487), (1, 'azul', 345), (2, 'verde', 783)]).toDF('id', 'color', 'importe')\n",
        "\n",
        "dataframe.show()\n",
        "\n",
        "#Seleccionas que se eliminen los duplicados en caso de una columna en concreto\n",
        "dataframe.dropDuplicates(['id', 'color']).show()"
      ],
      "metadata": {
        "id": "A2wH99RASARC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sort y orderby"
      ],
      "metadata": {
        "id": "FuBFZRk0Twod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = (spark.read.parquet('./data')\n",
        "    .select(col('likes'), col('views'), col('video_id'), col('dislikes'))\n",
        "    .dropDuplicates(['video_id'])\n",
        ")\n",
        "\n",
        "df.show()\n",
        "\n",
        "# sort\n",
        "\n",
        "df.sort('likes').show()\n",
        "\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "df.sort(desc('likes')).show()\n",
        "\n",
        "# función orderBy\n",
        "\n",
        "df.orderBy(col('views')).show()\n",
        "\n",
        "df.orderBy(col('views').desc()).show()\n",
        "\n",
        "dataframe = spark.createDataFrame([(1, 'azul', 568), (2, 'rojo', 235), (1, 'azul', 456), (2, 'azul', 783)]).toDF('id', 'color', 'importe')\n",
        "\n",
        "dataframe.show()\n",
        "\n",
        "\n",
        "##Ordena bajo las dos culumnas con la primera con prioridad\n",
        "dataframe.orderBy(col('color').desc(), col('importe')).show()\n",
        "\n",
        "# funcion limit, limita la cantidad de resultados\n",
        "\n",
        "top_10 = df.orderBy(col('views').desc()).limit(10)\n",
        "\n",
        "top_10.show()"
      ],
      "metadata": {
        "id": "kXp2W8l3TzP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Withcolum me lo salte"
      ],
      "metadata": {
        "id": "86srmQbmVrb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drop sample y random split"
      ],
      "metadata": {
        "id": "VRwpCBNxVttI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('./data')\n",
        "\n",
        "# drop, elimina columnas\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df_util = df.drop('comments_disabled')\n",
        "\n",
        "df_util.printSchema()\n",
        "\n",
        "df_util = df.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link')\n",
        "\n",
        "df_util.printSchema()\n",
        "\n",
        "df_util = df.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link', 'cafe')\n",
        "\n",
        "df_util.printSchema()\n",
        "\n",
        "# sample\n",
        "\n",
        "df_muestra = df.sample(0.8)\n",
        "\n",
        "num_filas = df.count()\n",
        "num_filas_muestra = df_muestra.count()\n",
        "\n",
        "print('El 80% de filas del dataframe original es {}'.format(num_filas - (num_filas*0.2)))\n",
        "print('El numero de filas del dataframe muestra es {}'.format(num_filas_muestra))\n",
        "\n",
        "df_muestra = df.sample(fraction=0.8, seed=1234)\n",
        "\n",
        "df_muestra = df.sample(withReplacement=True, fraction=0.8, seed=1234)\n",
        "\n",
        "# randomSplit\n",
        "\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "train, validation, test = df.randomSplit([0.6, 0.2, 0.2], seed=1234)\n",
        "\n",
        "train.count()\n",
        "\n",
        "validation.count()\n",
        "\n",
        "test.count()"
      ],
      "metadata": {
        "id": "eCwUjWeFV2SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Datos faltantes o incorrectos"
      ],
      "metadata": {
        "id": "f9DxcJTnYS3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('./data/')\n",
        "\n",
        "df.count()\n",
        "#Elimina las filas que tienen un valor null\n",
        "df.na.drop().count()\n",
        "\n",
        "df.na.drop('any').count()\n",
        "\n",
        "df.dropna().count()\n",
        "\n",
        "df.na.drop(subset=['views']).count()\n",
        "\n",
        "df.na.drop(subset=['views', 'dislikes']).count()\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()\n",
        "\n",
        "df.fillna(0).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()\n",
        "\n",
        "df.fillna(0, subset=['likes', 'dislikes']).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()"
      ],
      "metadata": {
        "id": "N5oLUV3VYVnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spark"
      ],
      "metadata": {
        "id": "8FeBm4jaKkj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('./data/')\n",
        "\n",
        "# show\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.show(5)\n",
        "\n",
        "#hace que se vean las lineas enteras\n",
        "df.show(5, truncate=False)\n",
        "\n",
        "# take, te da la fila directamente\n",
        "\n",
        "df.take(1)\n",
        "\n",
        "# head\n",
        "\n",
        "df.head(1)\n",
        "\n",
        "# collect\n",
        "\n",
        "df.select('likes').collect()"
      ],
      "metadata": {
        "id": "nHVcOW19BUs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Escritura de Dataframes"
      ],
      "metadata": {
        "id": "3bu6RTI3B_u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet('./data/')\n",
        "#Guardas el df en un archivo .csv y se cra un archivo por cada una de las particiones que tenga dl df\n",
        "df1 = df.repartition(2)\n",
        "\n",
        "df1.write.format('csv').option('sep', '|').save()\n",
        "\n",
        "df1.coalesce(1).write.format('csv').option('sep', '|').save('./output/csv1')\n",
        "\n",
        "df.printSchema()\n",
        "##Enseña todas las lineas diferentes quqe hay en commets disabled\n",
        "#En este caso para comprobar por que solo tendria que haber true or false\n",
        "df.select('comments_disabled').distinct().show()\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "#Solo se mantienen las lineas que comments.. tengan true or false\n",
        "df_limpio = df.filter(col('comments_disabled').isin('True', 'False'))\n",
        "\n",
        "df_limpio.write.partitionBy('comments_disabled').parquet('./output/parquet')"
      ],
      "metadata": {
        "id": "J4lfYLSwB9CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Buscar en internet conexion y creacion de un bucket en aws o gcs o azure blob"
      ],
      "metadata": {
        "id": "7I12pIJRMOVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Persistir"
      ],
      "metadata": {
        "id": "cymUgTk1CH85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'valor'])\n",
        "\n",
        "df.show()\n",
        "## hace que los cambios que vayas haciendo se van guardando\n",
        "df.persist()\n",
        "\n",
        "df.unpersist()\n",
        "\n",
        "df.cache()\n",
        "\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "df.persist(StorageLevel.DISK_ONLY)\n",
        "\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)"
      ],
      "metadata": {
        "id": "ioFoAY13CJud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ejercicios"
      ],
      "metadata": {
        "id": "OL4b2sxvKeXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark.read.parquet('./data')\n",
        "    .select(col('likes'), col('views'), col('video_id'), col('dislikes'))\n",
        "    .dropDuplicates(['video_id'])\n",
        ")\n",
        "\n",
        "df.show()\n",
        "\n",
        "# sort\n",
        "\n",
        "df.select('likes','dislikes').sort('ciudades').show(3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_sin_duplicados = df.distinct()\n",
        "df.filter(col('infected_id') == 'true' | col('infected_id') == 'false').count().show()\n",
        "\n",
        "df.filter(col('fem') =='fem').selct().show..."
      ],
      "metadata": {
        "id": "a3s3bmFrKg8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Capitulo 8 No se de que va PRO"
      ],
      "metadata": {
        "id": "XTZMy_JKV0vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aqui ya se ejecutan ocn funciones mas complejas\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n"
      ],
      "metadata": {
        "id": "m3oDF20jV5Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funciones count, countDistinct y approx_count_distinct"
      ],
      "metadata": {
        "id": "_Pr9MY5fXliL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones count, countDistinct y approx_count_distinct\n",
        "\n",
        "# count\n",
        "df = spark.read.parquet('./data/dataframe')\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "#Crea un df solo de los dos valores de count de colores y nombres sin contar nulos\n",
        "\n",
        "df.select(\n",
        "    count('nombre').alias('conteo_nombre'),\n",
        "    count('color').alias('conteo_color')\n",
        ").show()\n",
        "\n",
        "# realiza * un conteo de todas las filas\n",
        "df.select(\n",
        "    count('nombre').alias('conteo_nombre'),\n",
        "    count('color').alias('conteo_color'),\n",
        "    count('*').alias('conteo_general')\n",
        ").show()\n",
        "\n",
        "# countDistinct\n",
        "\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "df.select(\n",
        "    countDistinct('color').alias('colores_dif')\n",
        ").show()\n",
        "\n",
        "# approx_count_distinct, da una aproximacion por que una exacta podria requerir mucha carga  de trabajo en conjunto datos grandes\n",
        "\n",
        "from pyspark.sql.functions import approx_count_distinct\n",
        "\n",
        "dataframe = spark.read.parquet('./data/vuelos')\n",
        "\n",
        "dataframe.printSchema()\n",
        "\n",
        "dataframe.select(\n",
        "    countDistinct('AIRLINE'),\n",
        "    approx_count_distinct('AIRLINE')\n",
        ").show()\n"
      ],
      "metadata": {
        "id": "D87rGP3HXpc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funciones min y max"
      ],
      "metadata": {
        "id": "isO2i-Mnascl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max, col\n",
        "\n",
        "vuelos.select(\n",
        "    min('AIR_TIME').alias('menor_timepo'),\n",
        "    max('AIR_TIME').alias('mayor_tiempo')\n",
        ").show()\n",
        "\n",
        "vuelos.select(\n",
        "    min('AIRLINE_DELAY'),\n",
        "    max('AIRLINE_DELAY')\n",
        ").show()"
      ],
      "metadata": {
        "id": "H99JDZ1zaubI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funciones sum, sumDistinct y avg"
      ],
      "metadata": {
        "id": "iA-cRTCabcM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones sum, sumDistinct y avg\n",
        "\n",
        "from pyspark.sql.functions import sum, sumDistinct, avg, count\n",
        "\n",
        "# sum\n",
        "\n",
        "vuelos.printSchema()\n",
        "\n",
        "vuelos.select(\n",
        "    sum('DISTANCE').alias('sum_dis')\n",
        ").show()\n",
        "\n",
        "# sumDistinct\n",
        "\n",
        "vuelos.select(\n",
        "    sumDistinct('DISTANCE').alias('sum_dis_dif')\n",
        ").show()\n",
        "\n",
        "# avg\n",
        "\n",
        "vuelos.select(\n",
        "    avg('AIR_TIME').alias('promedio_aire'),\n",
        "    (sum('AIR_TIME') / count('AIR_TIME')).alias('prom_manual')\n",
        ").show()\n",
        "\n"
      ],
      "metadata": {
        "id": "bGfFLruNbcud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agregación con agrupación"
      ],
      "metadata": {
        "id": "t7fPmOCAcY46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregación con agrupación\n",
        "\n",
        "\n",
        "\n",
        "vuelos = spark.read.parquet('./data/')\n",
        "\n",
        "vuelos.printSchema()\n",
        "\n",
        "# Argupa cada aeropuerto y le hace un conteo de cada uno de ellos\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "(vuelos.groupBy('ORIGIN_AIRPORT')\n",
        "    .count()\n",
        "    .orderBy(desc('count'))\n",
        ").show()\n",
        "\n",
        "(vuelos.groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\n",
        "    .count()\n",
        "    .orderBy(desc('count'))\n",
        ").show()\n"
      ],
      "metadata": {
        "id": "Gc2UfXs2cZcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Varias agregaciones por grupo IMPORTANTE"
      ],
      "metadata": {
        "id": "cb79JPUEsT6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vuelos = spark.read.parquet('./data/')\n",
        "\n",
        "from pyspark.sql.functions import count, min, max, desc, avg\n",
        "\n",
        "vuelos.groupBy('ORIGIN_AIRPORT').agg(\n",
        "    count('AIR_TIME').alias('tiempo_aire'),\n",
        "    min('AIR_TIME').alias('min'),\n",
        "    max('AIR_TIME').alias('max')\n",
        ").orderBy(desc('tiempo_aire')).show()\n",
        "\n",
        "vuelos.groupBy('MONTH').agg(\n",
        "    count('ARRIVAL_DELAY').alias('conteo_de_retrasos'),\n",
        "    avg('DISTANCE').alias('prom_dist')\n",
        ").orderBy(desc('conteo_de_retrasos')).show()"
      ],
      "metadata": {
        "id": "qflX7SpbsWcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agregaciones con pivote"
      ],
      "metadata": {
        "id": "YfJnc14dtm9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max, avg, col\n",
        "\n",
        "estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso')).show()\n",
        "\n",
        "estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso'), min('peso'), max('peso')).show()\n",
        "\n",
        "estudiantes.groupBy('graduacion').pivot('sexo', ['M']).agg(avg('peso'), min('peso'), max('peso')).show()\n",
        "\n",
        "estudiantes.groupBy('graduacion').pivot('sexo', ['F']).agg(avg('peso'), min('peso'), max('peso')).show()"
      ],
      "metadata": {
        "id": "JYaATM7ItsuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Join de tablas"
      ],
      "metadata": {
        "id": "lQSO9DVlxrP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###inner join"
      ],
      "metadata": {
        "id": "cmWpRNa9Au-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##iner join\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "join_df = empleados.join(departamentos, col('num_dpto') == col('id'))\n",
        "\n",
        "join_df.show()\n",
        "\n",
        "join_df = empleados.join(departamentos, col('num_dpto') == col('id'), 'inner')\n",
        "\n",
        "join_df.show()\n",
        "\n",
        "join_df = empleados.join(departamentos).where(col('num_dpto') == col('id'))\n",
        "\n",
        "join_df.show()"
      ],
      "metadata": {
        "id": "omUJaACwxtnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Left Outer Join y anti join"
      ],
      "metadata": {
        "id": "hBVrOD1bAzzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "empleados.join(departamentos, col('num_dpto') == col('id'), 'leftouter').show()\n",
        "\n",
        "empleados.join(departamentos, col('num_dpto') == col('id'), 'left_outer').show()\n",
        "\n",
        "empleados.join(departamentos, col('num_dpto') == col('id'), 'left').show()\n",
        "\n",
        "\n",
        "#El anti join es lo mismo que el outer join de cada lado pero al contrario\n",
        "empleados.join(departamentos, col('num_dpto') == col('id'), 'outer').show()\n",
        "\n",
        "#Semi join hace casi lo mismo que el inner join"
      ],
      "metadata": {
        "id": "Rz9zCR2CA3nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Columnas repetidas"
      ],
      "metadata": {
        "id": "XEJAXi3DkcyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "depa = departamentos.withColumn('num_dpto', col('id'))\n",
        "\n",
        "depa.printSchema()\n",
        "\n",
        "empleados.printSchema()\n",
        "\n",
        "# Devuelve un error\n",
        "\n",
        "empleados.join(depa, col('num_dpto') == col('num_dpto'))\n",
        "\n",
        "# Forma correcta\n",
        "\n",
        "df_con_duplicados = empleados.join(depa, empleados['num_dpto'] == depa['num_dpto'])\n",
        "\n",
        "df_con_duplicados.printSchema()\n",
        "\n",
        "df_con_duplicados.select(empleados['num_dpto']).show()\n",
        "\n",
        "df2 = empleados.join(depa, 'num_dpto')\n",
        "\n",
        "df2.printSchema()\n",
        "\n",
        "empleados.join(depa, ['num_dpto']).printSchema()"
      ],
      "metadata": {
        "id": "bz5SxF1TkgLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Shuffle has join"
      ],
      "metadata": {
        "id": "wPaw3a3H5Pjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empleados = spark.read.parquet('./data/empleados/')\n",
        "\n",
        "departamentos = spark.read.parquet('./data/departamentos/')\n",
        "\n",
        "from pyspark.sql.functions import col, broadcast\n",
        "\n",
        "empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).show()\n",
        "\n",
        "empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).explain()"
      ],
      "metadata": {
        "id": "FmoG92Jq5T9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ejercicios"
      ],
      "metadata": {
        "id": "RnNhqxr-7x-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.read.option('header', 'true').csv('./players.csv')\n",
        "\n",
        "\n",
        "(df1.groupBy('country_of_citizenship').count().orderBy(desc('count'))).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_WmvHvy7z0e",
        "outputId": "3625892a-d224-4d33-ef32-2914d77353a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+-----+\n",
            "|country_of_citizenship|count|\n",
            "+----------------------+-----+\n",
            "|                 Spain| 1949|\n",
            "|                 Italy| 1884|\n",
            "|                France| 1758|\n",
            "|                Brazil| 1621|\n",
            "|               England| 1585|\n",
            "|               Ukraine| 1490|\n",
            "|                Russia| 1486|\n",
            "|           Netherlands| 1438|\n",
            "|                Turkey| 1416|\n",
            "|               Germany| 1333|\n",
            "|                Greece| 1228|\n",
            "|              Portugal| 1168|\n",
            "|               Belgium| 1118|\n",
            "|               Denmark| 1112|\n",
            "|              Scotland|  965|\n",
            "|             Argentina|  598|\n",
            "|               Türkiye|  585|\n",
            "|                Serbia|  384|\n",
            "|                  null|  380|\n",
            "|               Nigeria|  323|\n",
            "+----------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dfplayers = spark.read.option('header', 'true').csv('./players.csv')\n",
        "#dfplayers.printSchema()\n",
        "\n",
        "dfap = spark.read.option('header', 'true').csv('./appearances.csv')\n",
        "#dfap.printSchema()\n",
        "#dfap.show()\n",
        "\n",
        "dfap.groupBy('player_name').agg(sum('goals').alias('goals'),sum('assists').alias('assists'),sum('minutes_played').alias('minutes_played'),(sum('goals') / sum('minutes_played')).alias('goals_per_minute') ).orderBy(desc('goals')).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XejhB9Jo_aPn",
        "outputId": "b3d0a132-c1df-4221-a586-63a1298be7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+-------+--------------+--------------------+\n",
            "|         player_name|goals|assists|minutes_played|    goals_per_minute|\n",
            "+--------------------+-----+-------+--------------+--------------------+\n",
            "|  Robert Lewandowski|471.0|  116.0|       48131.0|0.009785792940100974|\n",
            "|        Lionel Messi|451.0|  224.0|       44380.0|0.010162235241099594|\n",
            "|   Cristiano Ronaldo|432.0|  114.0|       41150.0|0.010498177399756987|\n",
            "|         Luis Suárez|316.0|  160.0|       43629.0|0.007242888904169245|\n",
            "|          Harry Kane|314.0|   75.0|       37076.0|0.008469090516776351|\n",
            "|Pierre-Emerick Au...|294.0|   80.0|       38613.0|0.007614016004972419|\n",
            "|       Mohamed Salah|264.0|  125.0|       38183.0|0.006914071707304...|\n",
            "|       Ciro Immobile|259.0|   67.0|       34661.0|0.007472375292115058|\n",
            "|       Karim Benzema|257.0|  122.0|       35515.0|0.007236378994790...|\n",
            "|       Romelu Lukaku|257.0|   77.0|       38528.0|0.006670473421926911|\n",
            "|       Kylian Mbappé|244.0|  101.0|       24866.0| 0.00981259551194402|\n",
            "|   Wissam Ben Yedder|237.0|   72.0|       33446.0|0.007086049153859953|\n",
            "|      Edinson Cavani|232.0|   47.0|       27906.0| 0.00831362431018419|\n",
            "|   Antoine Griezmann|223.0|  107.0|       41244.0|0.005406847056541557|\n",
            "|          Edin Dzeko|220.0|  104.0|       36364.0|0.006049939500604994|\n",
            "| Alexandre Lacazette|220.0|   74.0|       34773.0|0.006326747764069...|\n",
            "|       Sergio Agüero|220.0|   57.0|       22962.0| 0.00958104694713004|\n",
            "|        Mauro Icardi|218.0|   62.0|       29929.0| 0.00728390524240703|\n",
            "|              Neymar|212.0|  143.0|       28364.0| 0.00747426315047243|\n",
            "|        Luuk de Jong|206.0|   99.0|       33310.0|0.006184329030321225|\n",
            "+--------------------+-----+-------+--------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dpp = spark.read.option('header', 'true').csv('./clubs.csv')\n",
        "dpp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrEgm6zrP1Jv",
        "outputId": "4e845e75-fde4-434c-bd55-2dc85685b2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+--------------------+-----------------------+------------------+----------+-----------+-----------------+---------------------+---------------------+--------------------+-------------+-------------------+----------+-----------+--------------------+--------------------+\n",
            "|club_id|          club_code|                name|domestic_competition_id|total_market_value|squad_size|average_age|foreigners_number|foreigners_percentage|national_team_players|        stadium_name|stadium_seats|net_transfer_record|coach_name|last_season|            filename|                 url|\n",
            "+-------+-------------------+--------------------+-----------------------+------------------+----------+-----------+-----------------+---------------------+---------------------+--------------------+-------------+-------------------+----------+-----------+--------------------+--------------------+\n",
            "|    105|    sv-darmstadt-98|     SV Darmstadt 98|                     L1|              null|        27|       25.6|               13|                 48.1|                    1|Merck-Stadion am ...|        17810|            +€3.05m|      null|       2023|../data/raw/trans...|https://www.trans...|\n",
            "|  11127|  ural-ekaterinburg|  Ural Yekaterinburg|                    RU1|              null|        30|       26.5|               11|                 36.7|                    3| Yekaterinburg Arena|        23000|             +€880k|      null|       2023|../data/raw/trans...|https://www.trans...|\n",
            "|    114|  besiktas-istanbul|Beşiktaş Jimnasti...|                    TR1|              null|        35|       25.7|               18|                 51.4|                   12|     Tüpraş Stadyumu|        42445|           €-18.65m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|     12|             as-rom|Associazione Spor...|                    IT1|              null|        24|       26.4|               18|                 75.0|                   18|    Olimpico di Roma|        73261|           €-64.10m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|    148|  tottenham-hotspur|Tottenham Hotspur...|                    GB1|              null|        25|       25.2|               18|                 72.0|                   16|Tottenham Hotspur...|        62850|           €-93.55m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|    157|           kaa-gent|Koninklijke Atlet...|                    BE1|              null|        27|       24.5|               19|                 70.4|                    9|  Planet Group Arena|        20185|           +€11.08m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|   1894|        hvidovre-if|         Hvidovre IF|                    DK1|              null|        25|       26.4|                3|                 12.0|                    2|PRO VENTILATION A...|        12000|             +€225k|      null|       2023|../data/raw/trans...|https://www.trans...|\n",
            "|    190|      fc-kopenhagen|Football Club Køb...|                    DK1|              null|        28|       25.2|               17|                 60.7|                   10|              Parken|        38065|           +€36.89m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|    192|   roda-jc-kerkrade|    Roda JC Kerkrade|                    NL1|              null|        25|       24.0|                9|                 36.0|                    0|Parkstad Limburg ...|        19979|            +€1.30m|      null|       2017|../data/raw/trans...|https://www.trans...|\n",
            "|  19789|   yeni-malatyaspor|    Yeni Malatyaspor|                    TR1|              null|        10|       22.9|                1|                 10.0|                    0|Yeni Malatya Stad...|        25745|             +€778k|      null|       2021|../data/raw/trans...|https://www.trans...|\n",
            "|   2079|          veria-nps|           Veria NPS|                    GR1|              null|        29|       24.3|                4|                 13.8|                    0|Dimotiko Stadio V...|         6350|                +-0|      null|       2016|../data/raw/trans...|https://www.trans...|\n",
            "|  21957|       ao-platanias|        AO Platanias|                    GR1|              null|         0|       null|                0|                 null|                    0|Dimotiko Gipedo P...|         3700|                +-0|      null|       2017|../data/raw/trans...|https://www.trans...|\n",
            "|   2414|         ac-horsens|          AC Horsens|                    DK1|              null|        19|       25.1|                9|                 47.4|                    4|Nordstern Arena H...|        10400|                +-0|      null|       2022|../data/raw/trans...|https://www.trans...|\n",
            "|   2457|     belenenses-sad|               B SAD|                    PO1|              null|        25|       24.2|               13|                 52.0|                    4|Estádio Nacional ...|        37593|            +€1.80m|      null|       2021|../data/raw/trans...|https://www.trans...|\n",
            "|   2578|    st-johnstone-fc|Saint Johnstone F...|                    SC1|              null|        26|       24.8|               16|                 61.5|                    3|      McDiarmid Park|        10696|                +-0|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|    269|      holstein-kiel|Kieler Sportverei...|                     L1|              null|        28|       25.2|               10|                 35.7|                    3|    Holstein-Stadion|        15034|            €-2.20m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|   2727|oud-heverlee-leuven| Oud-Heverlee Leuven|                    BE1|              null|        23|       24.5|               13|                 56.5|                    3|King Power at Den...|         9809|            +€8.25m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|  28643|   waasland-beveren|          SK Beveren|                    BE1|              null|        27|       26.8|               16|                 59.3|                    3|    Freethielstadion|        13290|             €-350k|      null|       2020|../data/raw/trans...|https://www.trans...|\n",
            "|   2990|  academica-coimbra|   Académica Coimbra|                    PO1|              null|        26|       25.2|                6|                 23.1|                    0|Estádio Cidade de...|        29744|             +€150k|      null|       2015|../data/raw/trans...|https://www.trans...|\n",
            "|   3057|   standard-luttich|Royal Standard Cl...|                    BE1|              null|        27|       23.1|               16|                 59.3|                    8|Maurice Dufrasne ...|        27221|            +€8.45m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "+-------+-------------------+--------------------+-----------------------+------------------+----------+-----------+-----------------+---------------------+---------------------+--------------------+-------------+-------------------+----------+-----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, sumDistinct, avg, count, desc, col\n",
        "\n",
        "#dfap1.show()\n",
        "#dfap1.groupBy('player_name').agg(sum('red_cards').alias('red_cards')).orderBy(desc('red_cards')).show()\n",
        "\n",
        "#dfap1.join(dpp, col('player_club_id') == col('club_id')).groupBy('player_id', 'player_club_id').agg(sum('red_cards').alias('red_cards')).select(col('player_name'),col('club_code'),col('red_cards')).orderBy(desc('red_cards')).show()\n",
        "#dfap1.join(dpp, col('player_club_id') == col('club_id')).show()\n",
        "# sum. .select(col('player_name'), col('club_code'))\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "dfap1.join(dpp, col('player_club_id') == col('club_id')).groupBy('player_id', 'player_club_id', 'player_name', 'name').agg(sum('red_cards').alias('red_cards')).select(col('red_cards'),col('name'),col('player_name')).orderBy(desc('red_cards')).show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-pxEgYU_wN-",
        "outputId": "252b0049-7034-4a87-96d0-491fa29e56c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+---------------+\n",
            "|red_cards|                name|    player_name|\n",
            "+---------+--------------------+---------------+\n",
            "|      4.0|Real Betis Balomp...|Germán Pezzella|\n",
            "|      4.0|Getafe Club de Fú...|          Djené|\n",
            "|      3.0|Real Sociedad de ...|Aritz Elustondo|\n",
            "|      3.0|Club Atlético de ...|   Ángel Correa|\n",
            "|      3.0|Getafe Club de Fú...|  Damián Suárez|\n",
            "|      3.0|Getafe Club de Fú...|   Alexis Ruano|\n",
            "|      3.0|Real Club Celta d...|   Rubén Blanco|\n",
            "|      3.0|           Málaga CF|           Duda|\n",
            "|      3.0|Sevilla Fútbol Cl...|   Marcos Acuña|\n",
            "|      3.0|Real Betis Balomp...|    Luiz Felipe|\n",
            "|      3.0|Reial Club Deport...|    Sergi Gómez|\n",
            "|      3.0|Club Atlético Osa...|    Chimy Ávila|\n",
            "|      3.0|Real Betis Balomp...|    Nabil Fekir|\n",
            "|      3.0|            Cádiz CF|   Iza Carcelén|\n",
            "|      3.0|Getafe Club de Fú...|     Allan Nyom|\n",
            "|      2.0|Valencia Club de ...|          Jonas|\n",
            "|      2.0|          Levante UD|  David Navarro|\n",
            "|      2.0|Athletic Club Bilbao|Aymeric Laporte|\n",
            "|      2.0|Real Club Celta d...|     Iago Aspas|\n",
            "|      2.0|Rayo Vallecano de...| Rubén Martínez|\n",
            "+---------+--------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfap1.join(dpp, dfap1.player_club_id == dpp.club_id) \\\n",
        "    .groupBy('player_id', 'player_club_id', 'player_name', 'club_code') \\\n",
        "    .agg(F.sum('red_cards').alias('red_cards')) \\\n",
        "    .orderBy(F.desc('red_cards')) \\\n",
        "    .select('player_name', 'club_code', 'red_cards') \\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "Dk67ftlBXVpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max, col, count\n",
        "dfjuegos = spark.read.option('header', 'true').csv('./games.csv')\n",
        "#dfjuegos.show()\n",
        "\n",
        "dfjuegos.select(\n",
        "    min('season'),\n",
        "    max('season'),\n",
        ").show()\n",
        "\n",
        "dfjuegos.groupBy('competition_id').agg(\n",
        "    count('game_id').alias('partidos')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xde6v5sWFR22",
        "outputId": "707dabb8-96a3-4580-f1e5-0ef3342a6305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|min(season)|max(season)|\n",
            "+-----------+-----------+\n",
            "|       2012|       2024|\n",
            "+-----------+-----------+\n",
            "\n",
            "+--------------+--------+\n",
            "|competition_id|partidos|\n",
            "+--------------+--------+\n",
            "|           CLQ|    1140|\n",
            "|           SUC|      28|\n",
            "|           IT1|    4590|\n",
            "|           CGB|     241|\n",
            "|           BE1|    3115|\n",
            "|          POSU|      12|\n",
            "|          UCOL|     141|\n",
            "|          FRCH|      12|\n",
            "|           SFA|    1134|\n",
            "|           CIT|     864|\n",
            "|          KLUB|      93|\n",
            "|           RUP|    1410|\n",
            "|            CL|    1494|\n",
            "|           USC|      13|\n",
            "|          UKRP|     587|\n",
            "|           NLP|    1030|\n",
            "|           DFL|      13|\n",
            "|           SCI|      14|\n",
            "|           GR1|    2742|\n",
            "|           GRP|    1081|\n",
            "+--------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dpp = spark.read.option('header', 'true').csv('./clubs.csv')\n",
        "dpp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_ktGoZGPSyS",
        "outputId": "7bfffe97-108e-4e66-a561-b43b9bbd9a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+--------------------+-----------------------+------------------+----------+-----------+-----------------+---------------------+---------------------+--------------------+-------------+-------------------+----------+-----------+--------------------+--------------------+\n",
            "|club_id|          club_code|                name|domestic_competition_id|total_market_value|squad_size|average_age|foreigners_number|foreigners_percentage|national_team_players|        stadium_name|stadium_seats|net_transfer_record|coach_name|last_season|            filename|                 url|\n",
            "+-------+-------------------+--------------------+-----------------------+------------------+----------+-----------+-----------------+---------------------+---------------------+--------------------+-------------+-------------------+----------+-----------+--------------------+--------------------+\n",
            "|    105|    sv-darmstadt-98|     SV Darmstadt 98|                     L1|              null|        27|       25.6|               13|                 48.1|                    1|Merck-Stadion am ...|        17810|            +€3.05m|      null|       2023|../data/raw/trans...|https://www.trans...|\n",
            "|  11127|  ural-ekaterinburg|  Ural Yekaterinburg|                    RU1|              null|        30|       26.5|               11|                 36.7|                    3| Yekaterinburg Arena|        23000|             +€880k|      null|       2023|../data/raw/trans...|https://www.trans...|\n",
            "|    114|  besiktas-istanbul|Beşiktaş Jimnasti...|                    TR1|              null|        35|       25.7|               18|                 51.4|                   12|     Tüpraş Stadyumu|        42445|           €-18.65m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|     12|             as-rom|Associazione Spor...|                    IT1|              null|        24|       26.4|               18|                 75.0|                   18|    Olimpico di Roma|        73261|           €-64.10m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|    148|  tottenham-hotspur|Tottenham Hotspur...|                    GB1|              null|        25|       25.2|               18|                 72.0|                   16|Tottenham Hotspur...|        62850|           €-93.55m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|    157|           kaa-gent|Koninklijke Atlet...|                    BE1|              null|        27|       24.5|               19|                 70.4|                    9|  Planet Group Arena|        20185|           +€11.08m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|   1894|        hvidovre-if|         Hvidovre IF|                    DK1|              null|        25|       26.4|                3|                 12.0|                    2|PRO VENTILATION A...|        12000|             +€225k|      null|       2023|../data/raw/trans...|https://www.trans...|\n",
            "|    190|      fc-kopenhagen|Football Club Køb...|                    DK1|              null|        28|       25.2|               17|                 60.7|                   10|              Parken|        38065|           +€36.89m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|    192|   roda-jc-kerkrade|    Roda JC Kerkrade|                    NL1|              null|        25|       24.0|                9|                 36.0|                    0|Parkstad Limburg ...|        19979|            +€1.30m|      null|       2017|../data/raw/trans...|https://www.trans...|\n",
            "|  19789|   yeni-malatyaspor|    Yeni Malatyaspor|                    TR1|              null|        10|       22.9|                1|                 10.0|                    0|Yeni Malatya Stad...|        25745|             +€778k|      null|       2021|../data/raw/trans...|https://www.trans...|\n",
            "|   2079|          veria-nps|           Veria NPS|                    GR1|              null|        29|       24.3|                4|                 13.8|                    0|Dimotiko Stadio V...|         6350|                +-0|      null|       2016|../data/raw/trans...|https://www.trans...|\n",
            "|  21957|       ao-platanias|        AO Platanias|                    GR1|              null|         0|       null|                0|                 null|                    0|Dimotiko Gipedo P...|         3700|                +-0|      null|       2017|../data/raw/trans...|https://www.trans...|\n",
            "|   2414|         ac-horsens|          AC Horsens|                    DK1|              null|        19|       25.1|                9|                 47.4|                    4|Nordstern Arena H...|        10400|                +-0|      null|       2022|../data/raw/trans...|https://www.trans...|\n",
            "|   2457|     belenenses-sad|               B SAD|                    PO1|              null|        25|       24.2|               13|                 52.0|                    4|Estádio Nacional ...|        37593|            +€1.80m|      null|       2021|../data/raw/trans...|https://www.trans...|\n",
            "|   2578|    st-johnstone-fc|Saint Johnstone F...|                    SC1|              null|        26|       24.8|               16|                 61.5|                    3|      McDiarmid Park|        10696|                +-0|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|    269|      holstein-kiel|Kieler Sportverei...|                     L1|              null|        28|       25.2|               10|                 35.7|                    3|    Holstein-Stadion|        15034|            €-2.20m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|   2727|oud-heverlee-leuven| Oud-Heverlee Leuven|                    BE1|              null|        23|       24.5|               13|                 56.5|                    3|King Power at Den...|         9809|            +€8.25m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "|  28643|   waasland-beveren|          SK Beveren|                    BE1|              null|        27|       26.8|               16|                 59.3|                    3|    Freethielstadion|        13290|             €-350k|      null|       2020|../data/raw/trans...|https://www.trans...|\n",
            "|   2990|  academica-coimbra|   Académica Coimbra|                    PO1|              null|        26|       25.2|                6|                 23.1|                    0|Estádio Cidade de...|        29744|             +€150k|      null|       2015|../data/raw/trans...|https://www.trans...|\n",
            "|   3057|   standard-luttich|Royal Standard Cl...|                    BE1|              null|        27|       23.1|               16|                 59.3|                    8|Maurice Dufrasne ...|        27221|            +€8.45m|      null|       2024|../data/raw/trans...|https://www.trans...|\n",
            "+-------+-------------------+--------------------+-----------------------+------------------+----------+-----------+-----------------+---------------------+---------------------+--------------------+-------------+-------------------+----------+-----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfjuegos.groupBy('competition_id').agg(\n",
        "    sum('attendance').alias('publico_total')).orderBy(desc('publico_total')).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHB-w1AwIItE",
        "outputId": "b8c6db83-4c70-4d82-840f-8188cd11701a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------+\n",
            "|competition_id|publico_total|\n",
            "+--------------+-------------+\n",
            "|           GB1| 1.54710808E8|\n",
            "|            L1| 1.34250002E8|\n",
            "|           ES1| 1.13305174E8|\n",
            "|           IT1|  9.9406514E7|\n",
            "|           FR1|  8.9093313E7|\n",
            "|            CL|  6.1081202E7|\n",
            "|           NL1|   6.033511E7|\n",
            "|            EL|  4.7293135E7|\n",
            "|           TR1|  3.5363822E7|\n",
            "|           PO1|  3.3905343E7|\n",
            "|           RU1|  3.3556227E7|\n",
            "|           BE1|  3.0541333E7|\n",
            "|           SC1|  3.0129945E7|\n",
            "|           FAC|  2.1558041E7|\n",
            "|           ELQ|  1.8991402E7|\n",
            "|           CLQ|  1.5814959E7|\n",
            "|           DFB|  1.4315035E7|\n",
            "|           DK1|  1.4192838E7|\n",
            "|           CDR|  1.2279002E7|\n",
            "|           GR1|  1.1137597E7|\n",
            "+--------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Capitulo 9\n"
      ],
      "metadata": {
        "id": "jX3ykFcg3E1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = spark.read.parquet('./data/convertir')\n",
        "\n",
        "data.printSchema()\n",
        "\n",
        "data.show(truncate=False)\n",
        "from pyspark.sql.functions import col, to_date, to_timestamp"
      ],
      "metadata": {
        "id": "lsLvO_J_4Pfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fecha y hora"
      ],
      "metadata": {
        "id": "bSr5pRNH4bXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones de fecha y hora\n",
        "# Ayuda para restar fechas...\n",
        "\n",
        "data1 = data.select(\n",
        "    to_date(col('date')).alias('date1'),\n",
        "    to_timestamp(col('timestamp')).alias('ts1'),\n",
        "    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n",
        "    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n",
        "\n",
        ")\n",
        "\n",
        "data1.show(truncate=False)\n",
        "\n",
        "data1.printSchema()\n",
        "\n",
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "data1.select(\n",
        "    date_format(col('date1'), 'dd-MM-yyyy')\n",
        ").show()\n",
        "\n",
        "df = spark.read.parquet('./data/calculo')\n",
        "\n",
        "df.show()\n",
        "\n",
        "##resta fechas\n",
        "from pyspark.sql.functions import datediff, months_between, last_day\n",
        "\n",
        "df.select(\n",
        "    col('nombre'),\n",
        "    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n",
        "    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n",
        "    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",
        ").show()\n",
        "\n",
        "\n",
        "#Añade dias a la fecha\n",
        "from pyspark.sql.functions import date_add, date_sub\n",
        "\n",
        "df.select(\n",
        "    col('nombre'),\n",
        "    col('fecha_ingreso'),\n",
        "    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n",
        "    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",
        ").show()\n",
        "\n",
        "from pyspark.sql.functions import year, month, dayofmonth, dayofyear, hour, minute, second\n",
        "\n",
        "df.select(\n",
        "    col('baja_sistema'),\n",
        "    year(col('baja_sistema')),\n",
        "    month(col('baja_sistema')),\n",
        "    dayofmonth(col('baja_sistema')),\n",
        "    dayofyear(col('baja_sistema')),\n",
        "    hour(col('baja_sistema')),\n",
        "    minute(col('baja_sistema')),\n",
        "    second(col('baja_sistema'))\n",
        ").show()\n"
      ],
      "metadata": {
        "id": "MUar9ZQT3KP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funciones String"
      ],
      "metadata": {
        "id": "tYVgHLVN4d-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.parquet('./data/')\n",
        "\n",
        "data.show()\n",
        "#Trim elimina espacios\n",
        "from pyspark.sql.functions import ltrim, rtrim, trim\n",
        "\n",
        "data.select(\n",
        "    ltrim('nombre').alias('ltrim'),\n",
        "    rtrim('nombre').alias('rtrim'),\n",
        "    trim('nombre').alias('trim')\n",
        ").show()\n",
        "\n",
        "from pyspark.sql.functions import col, lpad, rpad\n",
        "\n",
        "#Añade ese simbolo a la izq o dere hasta rellenar el taño dicho. Ejm 8\n",
        "data.select(\n",
        "    trim(col('nombre')).alias('trim')\n",
        ").select(\n",
        "    lpad(col('trim'), 8, '-').alias('lpad'),\n",
        "    rpad(col('trim'), 8, '=').alias('rpad')\n",
        ").show()\n",
        "\n",
        "df1 = spark.createDataFrame([('Spark', 'es', 'maravilloso')], ['sujeto', 'verbo', 'adjetivo'])\n",
        "\n",
        "df1.show()\n",
        "\n",
        "#Un select y luego otro selecct hace que se haga uno dentro de otro\n",
        "#Concat concatena, initcap es la primera letra de cada palabra en mayuscula\n",
        "from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse\n",
        "\n",
        "df1.select(\n",
        "    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase')\n",
        ").select(\n",
        "    col('frase'),\n",
        "    lower(col('frase')).alias('minuscula'),\n",
        "    upper(col('frase')).alias('mayuscula'),\n",
        "    initcap(col('frase')).alias('initcap'),\n",
        "    reverse(col('frase')).alias('reversa')\n",
        ").show()\n",
        "\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "df2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])\n",
        "\n",
        "df2.show(truncate=False)\n",
        "\n",
        "#Donde encuentre voy o por la cambia por ir\n",
        "df2.select(\n",
        "    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase')\n",
        ").show(truncate=False)"
      ],
      "metadata": {
        "id": "5WBFlcck4n1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Colecciones, listas arrays"
      ],
      "metadata": {
        "id": "pgp4T3MG4ty3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones para trabajo con colecciones\n",
        "\n",
        "\n",
        "data = spark.read.parquet('./data/parquet/')\n",
        "\n",
        "data.show(truncate=False)\n",
        "\n",
        "data.printSchema()\n",
        "\n",
        "#Hay un array en una columna, cuenta el tamaño, lo ordena alfabeticamente y busca\n",
        "from pyspark.sql.functions import col, size, sort_array, array_contains\n",
        "\n",
        "data.select(\n",
        "    size(col('tareas')).alias('tamaño'),\n",
        "    sort_array(col('tareas')).alias('arreglo_ordenado'),\n",
        "    array_contains(col('tareas'), 'buscar agua').alias('buscar_agua')\n",
        ").show(truncate=False)\n",
        "\n",
        "#Reparte cada tarea de la lista en otra\n",
        "\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "data.select(\n",
        "    col('dia'),\n",
        "    explode(col('tareas')).alias('tareas')\n",
        ").show()\n",
        "\n",
        "# Formato JSON\n",
        "\n",
        "json_df_str = spark.read.parquet('./data/JSON')\n",
        "\n",
        "json_df_str.show(truncate=False)\n",
        "\n",
        "json_df_str.printSchema()\n",
        "\n",
        "# Al obtenerlo de un jswon no se parsea bien\n",
        "#Lo que hay que hacer es crearle una estructura para que se puedan pasar y entender bien los datos\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
        "\n",
        "schema_json = StructType(\n",
        "    [\n",
        "     StructField('dia', StringType(), True),\n",
        "     StructField('tareas', ArrayType(StringType()), True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "from pyspark.sql.functions import from_json, to_json\n",
        "\n",
        "json_df = json_df_str.select(\n",
        "    from_json(col('tareas_str'), schema_json).alias('por_hacer')\n",
        ")\n",
        "\n",
        "json_df.printSchema()\n",
        "\n",
        "#Lo acabamos adaptando para obetner todos los datos de una columna\n",
        "json_df.select(\n",
        "    col('por_hacer').getItem('dia'),\n",
        "    col('por_hacer').getItem('tareas'),\n",
        "    col('por_hacer').getItem('tareas').getItem(0).alias('primer_tarea')\n",
        ").show(truncate=False)\n",
        "\n",
        "json_df.select(\n",
        "    to_json(col('por_hacer'))\n",
        ").show(truncate=False)\n"
      ],
      "metadata": {
        "id": "5uN_bxMU40eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funciones when, coalesce y lit"
      ],
      "metadata": {
        "id": "7AEBvQNV46JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones when, coalesce y lit\n",
        "\n",
        "\n",
        "data = spark.read.parquet('./data/')\n",
        "\n",
        "data.show()\n",
        "\n",
        "from pyspark.sql.functions import col, when, lit, coalesce\n",
        "\n",
        "data.select(\n",
        "    col('nombre'),\n",
        "    when(col('pago') == 1, 'pagado').when(col('pago') == 2, 'sin pagar').otherwise('sin iniciar').alias('pago')\n",
        ").show()\n",
        "# cambia los nulos por un texto\n",
        "data.select(\n",
        "    coalesce(col('nombre'), lit('sin nombre')).alias('nombre')\n",
        ").show()\n"
      ],
      "metadata": {
        "id": "vyr0hBfL46ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Funciones definidas por el usuario UDF IMPORTANTE"
      ],
      "metadata": {
        "id": "Ks9q9kaV5E8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones definidas por el usuario UDF IMPORTANTE\n",
        "\n",
        "def cubo(n):\n",
        "    return n * n * n\n",
        "\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "spark.udf.register('cubo', f_cubo, LongType())\n",
        "\n",
        "spark.range(1,10).createOrReplaceTempView('df_temp')\n",
        "\n",
        "spark.sql(\"SELECT id, cubo(id) AS cubo FROM df_temp\").show()\n",
        "\n",
        "def bienvenida(nombre):\n",
        "    return ('Hola {}'.format(nombre))\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "bienvenida_udf = udf(lambda x: bienvenida(x), StringType())\n",
        "\n",
        "df_nombre = spark.createDataFrame([('Jose',), ('Julia',)], ['nombre'])\n",
        "\n",
        "df_nombre.show()\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_nombre.select(\n",
        "    col('nombre'),\n",
        "    bienvenida_udf(col('nombre')).alias('bie_nombre')\n",
        ").show()\n",
        "\n",
        "#-----------------------------Opcion que mas me mola -----------------------------------#\n",
        "\n",
        "\n",
        "@udf(returnType=StringType())\n",
        "def mayuscula(s):\n",
        "    return s.upper()\n",
        "\n",
        "df_nombre.select(\n",
        "    col('nombre'),\n",
        "    mayuscula(col('nombre')).alias('may_nombre')\n",
        ").show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#-----------------------------Opcion mas rapida con pandas-----------------------------#\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "def cubo_pandas(a: pd.Series) -> pd.Series:\n",
        "    return a * a * a\n",
        "\n",
        "cubo_udf = pandas_udf(cubo_pandas, returnType=LongType())\n",
        "\n",
        "\n",
        "df = spark.range(5)\n",
        "\n",
        "df.select(\n",
        "    col('id'),\n",
        "    cubo_udf(col('id')).alias('cubo_pandas')\n",
        ").show()"
      ],
      "metadata": {
        "id": "E7M7LTQg5IYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Window partition by"
      ],
      "metadata": {
        "id": "6e3VgfzumoFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones de ventana\n",
        "#--------------Son formas de mostrar los datos particionados o agrupados --------------\n",
        "\n",
        "\n",
        "df = spark.read.parquet('./data/')\n",
        "\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import desc, row_number, rank, dense_rank, col\n",
        "\n",
        "\n",
        "#---------------------Particiona en base a los departamentos -------\n",
        "windowSpec = Window.partitionBy('departamento').orderBy(desc('evaluacion'))\n",
        "\n",
        "# row_number\n",
        "\n",
        "df.withColumn('row_number', row_number().over(windowSpec)).filter(col('row_number').isin(1,2)).show()\n",
        "\n",
        "# rank, ranquea cada posicion de cada numero en base a los grupos\n",
        "\n",
        "df.withColumn('rank', rank().over(windowSpec)).show()\n",
        "\n",
        "# dense_rank\n",
        "\n",
        "df.withColumn('dense_rank', dense_rank().over(windowSpec)).show()\n",
        "\n",
        "# Agregaciones con especificaciones de ventana\n",
        "\n",
        "windowSpecAgg = Window.partitionBy('departamento')\n",
        "\n",
        "from pyspark.sql.functions import min, max, avg\n",
        "\n",
        "(df.withColumn('min', min(col('evaluacion')).over(windowSpecAgg))\n",
        ".withColumn('max', max(col('evaluacion')).over(windowSpecAgg))\n",
        ".withColumn('avg', avg(col('evaluacion')).over(windowSpecAgg))\n",
        ".withColumn('row_number', row_number().over(windowSpec))\n",
        " ).show()"
      ],
      "metadata": {
        "id": "M_ViRVcAmrD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Catalyst Optimizer\n"
      ],
      "metadata": {
        "id": "fz8qeQVhon1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Catalyst Optimizer\n",
        "\n",
        "\n",
        "\n",
        "data = spark.read.parquet('./data/')\n",
        "\n",
        "data.printSchema()\n",
        "\n",
        "data.show()\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "nuevo_df = (data.filter(col('MONTH').isin(6,7,8))\n",
        "            .withColumn('dis_tiempo_aire', col('DISTANCE') / col('AIR_TIME'))\n",
        ").select(\n",
        "    col('AIRLINE'),\n",
        "    col('dis_tiempo_aire')\n",
        ").where(col('AIRLINE').isin('AA', 'DL', 'AS'))\n",
        "\n",
        "nuevo_df.explain(True)\n"
      ],
      "metadata": {
        "id": "V0LAVROCoocQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spark web UI"
      ],
      "metadata": {
        "id": "7uh3pifAsAoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder.config('spark.ui.port', '4050')\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "from google.colab import output\n",
        "\n",
        "output.serve_kernel_port_as_window(4050, path='/jobs.index.html')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "aq2OwYQPsHpo",
        "outputId": "bf886274-db86-4b10-ca10-bc2ebda74f5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-47078dbb44cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m spark = (\n\u001b[1;32m      4\u001b[0m     \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.ui.port'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'4050'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}